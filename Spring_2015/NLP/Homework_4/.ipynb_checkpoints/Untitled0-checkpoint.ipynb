{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Problem 1\n",
      "# P(Bob and Alice guess the same) = P(Bob guesses DT and Alice guesses DT) + P(Bob guesses NN and Alice guess NN) + P(Bob guesses VB and Alice guesses VB)\n",
      "# 0.3 * 0.3 + 0.5 * 0.5 + 0.2 + 0.2 = 0.38"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "11 Educators/NNP Guilty/JJ of/P Racketeering/V in/P Atlanta/NNP Testing/NN Case/NN\n",
      "Getty/NNP grandson/NN had/V restraining/V order/V against/P ex/NN before/P death/NN\n",
      "Serena/NNP sits/V at/P 700 after/JJ Miami/NNP quarterfinal/NN win/V"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-2-93195d3d2fd8>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-93195d3d2fd8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    11 Educators/NNP Guilty/JJ of/P Racketeering/V in/P Atlanta/NNP Testing/NN Case/NN\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "patterns = [(r'^(A|a)s|(L|l)ike$', 'CS'),       # conjunctions\n",
      "    (r'.*s$', 'NNS'),                   # plural nouns                  \n",
      "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),    ## cardinal numbers \n",
      "    (r'.+able$', 'JJ'),                 # a lot of adjectives end in 'able'\n",
      "    (r'.*ly$', 'RB'),                   # a lot of adverbs end in 'ly'\n",
      "    (r'.*er$', 'JJR'),                  # adjective, comparative#words ending in ing are labeled as gerunds\n",
      "    (r'^have$','HV'),\n",
      "    (r'^has$', 'HVZ'),\n",
      "    (r'(the|a|an)$', 'AT'),\n",
      "    (r'.*s\\'$', 'NN$')\n",
      "]\n",
      "tagger = nltk.RegexpTagger(pattern)\n",
      "tagger.tag(nltk.word_tokenize('He was borning in March 1991'))\n",
      "#tagger.tag(nltk.word_tokenize(str(brown.words(categories='adventure'))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[('He', None),\n",
        " ('was', None),\n",
        " ('borning', 'gerund VBG'),\n",
        " ('in', None),\n",
        " ('March', None),\n",
        " ('1991', None)]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hidden Markov Models in Python\n",
      "# Katrin Erk, March 2013\n",
      "#\n",
      "# This HMM addresses the problem of part-of-speech tagging. It estimates\n",
      "# the probability of a tag sequence for a given word sequence as follows:\n",
      "#\n",
      "# Say words = w1....wN\n",
      "# and tags = t1..tN\n",
      "#\n",
      "# then\n",
      "# P(tags | words) is_proportional_to  product P(ti | t{i-1}) P(wi | ti)\n",
      "#\n",
      "# To find the best tag sequence for a given sequence of words,\n",
      "# we want to find the tag sequence that has the maximum P(tags | words)\n",
      "import nltk\n",
      "import sys\n",
      "from nltk.corpus import brown\n",
      "import operator\n",
      "\n",
      "# Estimating P(wi | ti) from corpus data using Maximum Likelihood Estimation (MLE):\n",
      "# P(wi | ti) = count(wi, ti) / count(ti)\n",
      "#\n",
      "# We add an artificial \"start\" tag at the beginning of each sentence, and\n",
      "# We add an artificial \"end\" tag at the end of each sentence.\n",
      "# So we start out with the brown tagged sentences,\n",
      "# add the two artificial tags,\n",
      "# and then make one long list of all the tag/word pairs.\n",
      "\n",
      "brown_tags_words = [ ]\n",
      "for sent in brown.tagged_sents(simplify_tags = True):\n",
      "    # sent is a list of word/tag pairs\n",
      "    # add START/START at the beginning\n",
      "    brown_tags_words.append( (\"START\", \"START\") )\n",
      "    # then all the tag/word pairs for the word/tag pairs in the sentence\n",
      "    brown_tags_words.extend([ (tag, word) for (word, tag) in sent ])\n",
      "    # then END/END\n",
      "    brown_tags_words.append( (\"END\", \"END\") )\n",
      "\n",
      "# conditional frequency distribution\n",
      "cfd_tagwords = nltk.ConditionalFreqDist(brown_tags_words)\n",
      "# conditional probability distribution\n",
      "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
      "duck_prob = cpd_tagwords[\"V\"].prob(\"duck\")\n",
      "print \"the probability of duck is \" + str(duck_prob)\n",
      "# The probability is 7.99826323427e-05\n",
      "print \"The probability of an adjective (ADJ) being 'new' is\", cpd_tagwords[\"ADJ\"].prob(\"new\")\n",
      "\n",
      "# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n",
      "# P(ti | t{i-1}) = count(t{i-1}, ti) / count(t{i-1})\n",
      "brown_tags = [tag for (tag, word) in brown_tags_words ]\n",
      "\n",
      "# make conditional frequency distribution:\n",
      "# count(t{i-1} ti)\n",
      "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))\n",
      "# make conditional probability distribution, using\n",
      "# maximum likelihood estimate:\n",
      "# P(ti | t{i-1})\n",
      "cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
      "\n",
      "adj_prob = cpd_tags[\"V\"].prob(\"ADJ\")\n",
      "print \"The probability of 'adj' is \" + str(adj_prob) \n",
      "#probability is 0.0758006832802\n",
      "noun_prob = cpd_tags[\"V\"].prob(\"N\")\n",
      "print \"The probability of 'N' is \" + str(noun_prob)\n",
      "#probability is 0.0445160479439\n",
      "\n",
      "\n",
      "###\n",
      "# putting things together:\n",
      "# what is the probability of the tag sequence \"PRO V TO V\" for the word sequence \"I want to race\"?\n",
      "# It is\n",
      "# P(START) * P(PRO|START) * P(I | PRO) *\n",
      "#            P(V | PRO) * P(want | V) *\n",
      "#            P(TO | V) * P(to | TO) *\n",
      "#            P(VB | TO) * P(race | V) *\n",
      "#            P(END | V)\n",
      "\n",
      "prob_tagsequence = cpd_tags[\"START\"].prob(\"PRO\") * cpd_tagwords[\"PRO\"].prob(\"I\") * \\\n",
      "    cpd_tags[\"PRO\"].prob(\"V\") * cpd_tagwords[\"V\"].prob(\"saw\") * \\\n",
      "    cpd_tags[\"V\"].prob(\"PRO\") * cpd_tagwords[\"PRO\"].prob(\"her\") * \\\n",
      "    cpd_tags[\"PRO\"].prob(\"N\") * cpd_tagwords[\"N\"].prob(\"duck\") * \\\n",
      "    cpd_tags[\"N\"].prob(\"END\")\n",
      "print \"Probability of 'START PRO V PRO N END' for 'I saw her duck' is:\", prob_tagsequence \n",
      "# probability is 8.45304673083e-18\n",
      "\n",
      "#####\n",
      "# Viterbi:\n",
      "# If we have a word sequence, what is the best tag sequence?\n",
      "#\n",
      "# The method above lets us determine the probability for a single tag sequence.\n",
      "# But in order to find the best tag sequence, we need the probability\n",
      "# for _all_ tag sequence.\n",
      "# What Viterbi gives us is just a good way of computing all those many probabilities\n",
      "# as fast as possible.\n",
      "\n",
      "# what is the list of all tags?\n",
      "distinct_tags = set(brown_tags)\n",
      "#sentence = [\"I\", \"want\", \"to\", \"race\"]\n",
      "sentence = [\"I\", \"saw\", \"her\", \"duck\"]\n",
      "sentlen = len(sentence)\n",
      "# viterbi: this is a list. \n",
      "# for each step i in 1 .. sentlen,\n",
      "# store a dictionary\n",
      "# that maps each tag X\n",
      "# to the probability of the best tag sequence of length i that ends in X\n",
      "viterbi = [ ]\n",
      "# backpointer:\n",
      "# for each step i in 1..sentlen,\n",
      "# store a dictionary\n",
      "# that maps each tag X\n",
      "# to the previous tag in the best tag sequence of length i that ends in X\n",
      "backpointer = [ ]\n",
      "##\n",
      "# we first determine the viterby dictionary for the first word:\n",
      "# For each tag, what is the probability of it following \"START\" and for it\n",
      "# producing the first word of the sentence?\n",
      "first_viterbi = { }\n",
      "first_backpointer = { }\n",
      "for tag in distinct_tags:\n",
      "    # don't record anything for the START tag\n",
      "    if tag == \"START\": continue\n",
      "    first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[0] )\n",
      "    first_backpointer[ tag ] = \"START\"\n",
      "# store first_viterbi (the dictionary for the first word in the sentence)\n",
      "# in the viterbi list, and record that the best previous tag\n",
      "# for any first tag is \"START\"\n",
      "viterbi.append(first_viterbi)\n",
      "backpointer.append(first_backpointer)\n",
      "besttag = max(first_viterbi.iteritems(), key = operator.itemgetter(1))[0]\n",
      "print \"Word\", sentence[0], \"current best tag:\", besttag  #Pro, VD\n",
      "# now we iterate over all remaining words in the sentence, from the second to the last.\n",
      "for wordindex in range(1, len(sentence)):\n",
      "         # start a new dictionary where we can store, for each tag, the probability \n",
      "     # of the best tag sequence ending in that tag\n",
      "         # for the current word in the sentence\n",
      "    this_viterbi = { }\n",
      "         # start a new dictionary we we can store, for each tag,\n",
      "    # the best previous tag\n",
      "    this_backpointer = { }\n",
      "         # prev_viterbi is a dictionary that stores, for each tag, the probability\n",
      "    # of the best tag sequence ending in that tag\n",
      "    # for the previous word in the sentence.\n",
      "    # So it stores, for each tag, the probability of a tag sequence up to the previous word\n",
      "    # ending in that tag. \n",
      "    prev_viterbi = viterbi[-1]\n",
      "    \n",
      "         # for each tag, determine what the best previous-tag is,\n",
      "    # and what the probability is of the best tag sequence ending in this tag.\n",
      "    # store this information in the dictionary this_viterbi\n",
      "    for tag in distinct_tags:\n",
      "        # don't record anything for the START tag\n",
      "        if tag == \"START\": continue\n",
      "        # if this tag is X and the current word is w, then \n",
      "        # find the previous tag Y such that\n",
      "        # the best tag sequence that ends in X\n",
      "        # actually ends in Y X\n",
      "        # that is, the Y that maximizes\n",
      "        # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
      "        # The following command has the same notation\n",
      "        # that you saw in the sorted() command.\n",
      "        best_previous = max(prev_viterbi.keys(),\n",
      "                            key = lambda prevtag: \\\n",
      "            prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
      "        # Instead, we can also use the following longer code:\n",
      "        # best_previous = None\n",
      "        # best_prob = 0.0\n",
      "        # for prevtag in prev_viterbi.keys():\n",
      "        #    prob = prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex])\n",
      "        #    if prob >= best_prob:\n",
      "        #        best_previous= prevtag\n",
      "        #        best_prob = prob\n",
      "        #\n",
      "                  # this_viterbi[ tag ] is the probability of the best tag sequence ending in tag\n",
      "        this_viterbi[ tag ] = prev_viterbi[ best_previous] * \\\n",
      "            cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[wordindex])\n",
      "                 # this_backpointer[ tag ] is the most likely previous-tag for this current tag\n",
      "        this_backpointer[ tag ] = best_previous\n",
      "    # done with all tags in this iteration\n",
      "    # so store the current viterbi step\n",
      "    besttag = max(this_viterbi.iteritems(), key = operator.itemgetter(1))[0]\n",
      "    print \"Word\", sentence[wordindex], \"current best tag:\", besttag # PRO, V\n",
      "    viterbi.append(this_viterbi)\n",
      "    backpointer.append(this_backpointer)\n",
      "# done with all words in the sentence.\n",
      "# now find the probability of each tag\n",
      "# to have \"END\" as the next tag,\n",
      "# and use that to find the overall best sequence\n",
      "prev_viterbi = viterbi[-1]\n",
      "best_previous = max(prev_viterbi.keys(),\n",
      "                    key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
      "prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
      "# best tagsequence: we store this in reverse for now, will invert later\n",
      "best_tagsequence = [ \"END\", best_previous ]\n",
      "# invert the list of backpointers\n",
      "backpointer.reverse()\n",
      "# go backwards through the list of backpointers\n",
      "# (or in this case forward, because we have inverter the backpointer list)\n",
      "# in each case:\n",
      "# the following best tag is the one listed under\n",
      "# the backpointer for the current best tag\n",
      "current_best_tag = best_previous\n",
      "for bp in backpointer:\n",
      "    best_tagsequence.append(bp[current_best_tag])\n",
      "    current_best_tag = bp[current_best_tag]\n",
      "best_tagsequence.reverse()\n",
      "print \"The sentence was:\",\n",
      "for w in sentence: print w, #I saw her duck\n",
      "print\n",
      "print \"The best tag sequence is:\",\n",
      "for t in best_tagsequence: print t, # Best tag sequence: START PRO VD PRO N END\n",
      "print\n",
      "print \"The probability of the best tag sequence is:\", prob_tagsequence # Probability of sequence: 1.02492156447e-15"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "the probability of duck is 7.99826323427e-05\n",
        "The probability of an adjective (ADJ) being 'new' is 0.0147234491763\n",
        "The probability of 'adj' is 0.0758006832802"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The probability of 'N' is 0.0445160479439\n",
        "Probability of 'START PRO V PRO N END' for 'I saw her duck' is: 8.45304673083e-18\n",
        "Word I current best tag: PRO\n",
        "Word"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " want current best tag: V\n",
        "('V', 1.3187857665267122e-05)\n",
        "Word to current best tag: TO\n",
        "('TO', 3.613972041386485e-07)\n",
        "Word race current best tag: V\n",
        "('V', 1.6311483671331526e-11)\n",
        "The sentence was: I want to race\n",
        "The best tag sequence is: START PRO V P N END\n",
        "The probability of the best tag sequence is: 2.78524839629e-14\n",
        "Word I current best tag: PRO\n",
        "Word saw current best tag: VD\n",
        "Word her current best tag: PRO\n",
        "Word duck current best tag: V\n",
        "The sentence was:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " I saw her duck\n",
        "The best tag sequence is: START PRO VD PRO N END\n",
        "The probability of the best tag sequence is: 1.02492156447e-15\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}